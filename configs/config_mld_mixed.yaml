
NAME: DualMode_MLD_Finetune_v1_1105
# Debug 模式，设为 True 时会使用极少量数据，方便快速测试流程
DEBUG: False
ACCELERATOR: 'gpu'
DEVICE: [0]

#####################################
# 训练设置 (Training Settings)
#####################################
TRAIN:
  # 模型阶段，在我们的微调任务中，固定为 'diffusion'
  STAGE: diffusion
  # [核心] 数据集名称，'mixed' 将激活我们的 MixedDataModule
  DATASETS: ['mixed']
  # 数据加载器的工作线程数
  NUM_WORKERS: 8
  # 训练批次大小
  BATCH_SIZE: 256
  # 微调的总轮数
  END_EPOCH: 350

  # [核心] 指向你下载的、原始的 MCM-LDM Denoiser 预训练权重
  # 这是我们微调的起点
  PRETRAINED: "checkpoints/denoiser_checkpoint/denoiser.ckpt"
  # [核心] 指向预训练的 VAE 模型权重
  PRETRAINED_VAE: "checkpoints/vae_checkpoint/vae7.ckpt"
  # RESUME: '/root/autodl-tmp/MyRepository/MCM-LDM/experiments/mld/DualMode_MLD_Finetune_v1_1104/checkpoints/last.ckpt' # 如果需要从中断的训练中恢复，请填写此路径
  # PRETRAINED: '/root/autodl-tmp/MyRepository/MCM-LDM/experiments/mld/DualMode_MLD_Finetune_v1_1104/checkpoints/last.ckpt'
  RESUME: ''
  # [核心] 差分学习率配置
  OPTIM:
    TYPE: AdamW
    # 核心思路：我们现在有三组学习率，分别控制不同部分的学习速度。
    # 1. LR (基础学习率): 用于 Denoiser 的原始部分，需要非常小，因为它们已经预训练得很好。
    # 2. LR_NEW (新模块学习率): 用于 Denoiser 中新加的 adaLN_modulation_text，可以稍高，让它快速学习。
    # 3. LR_FINETUNE (微调学习率): 用于 CLIP 和 MotionClip，必须非常小，以避免灾难性遗忘。

    # [推荐值] Denoiser 基础参数的学习率
    LR: 1.0e-6 
    
    # [推荐值] Denoiser 新增模块 (adaLN_modulation_text) 的学习率
    LR_NEW: 5.0e-6  # 5.0e-6

    # [推荐值] 外部模型 (CLIP, MotionClip) 的微调学习率
    # LR_FINETUNE: 5.0e-7
    LR_CLIP: 5.0e-7      # 从 5.0e-7 降低 5 倍
    LR_MOTIONCLIP: 5.0e-7 # 从 5.0e-7 降低 5 倍

#####################################
# 验证设置 (Validation Settings)
#####################################
EVAL:
  # 验证集，我们的 DataModule 会使用 HumanML3D 的 val split
  # DATASETS: ['humanml3d']
  DATASETS: ['mixed']
  # 验证批次大小
  BATCH_SIZE: 256
  SPLIT: val # 固定为 'val'

#####################################
# 数据集设置 (Dataset Settings)
#####################################
DATASET:
  JOINT_TYPE: 'humanml3d'
  NFEATS: 263
  NJOINTS: 22
  # [核心] 激活 MixedDataModule 的开关
  TYPE: 'mixed'
  # [核心] 词向量化器 (Glove) 的路径
  WORD_VERTILIZER_PATH: "deps/t2m/glove/"

  # [核心] 混合数据集的配置
  MIXED:
    # 比例: [HumanML3D, 100Style]。加起来最好等于 BATCH_SIZE
    BATCH_RATIO: [128, 128] 

  # [核心] HumanML3D 数据集的配置
  HUMANML3D:
    ROOT: "/root/autodl-tmp/MyRepository/MCM-LDM/datasets/humanml3d"
    SAMPLER: {MAX_LEN: 196, MIN_LEN: 40, MAX_TEXT_LEN: 20}
    UNIT_LEN: 4
    MIN_FILTER_LEN: 40
    MAX_FILTER_LEN: 200 # 根据我们的数据分析结果设定
  
  # [核心] 100Style 数据集的配置
  STYLE100:
    ROOT: "/root/autodl-tmp/MyRepository/MCM-LDM/datasets/100StyleDataset"
    SAMPLER: {MAX_LEN: 196, MIN_LEN: 40, MAX_TEXT_LEN: 20}
    UNIT_LEN: 4
    MIN_FILTER_LEN: 40
    MAX_FILTER_LEN: 720 # 根据我们的数据分析结果设定

#####################################
# 模型设置 (Model Settings)
#####################################
model:
  # --- VAE 和 Denoiser 的骨架参数 (与原始配置保持一致) ---
  vae: true
  model_type: mld
  condition: 'text'
  latent_dim: [7, 256]
  ff_size: 1024
  num_layers: 9
  num_head: 4
  droupout: 0.1 # 原文是 dropout，这里保持拼写错误以兼容
  activation: gelu
  t2m_path: 'deps/t2m/t2m'
  
  # --- 推理时 Classifier-Free Guidance 的默认参数 ---
  guidance_scale: 5.0
  guidance_uncondp: 0.1

  # [核心] Denoiser 的专属配置，将通过 **kwargs 传入
  denoiser:
    # 激活我们新的 CrossAttention 通路
    # use_text_condition: True
    # CLIP 文本特征的维度 (ViT-B/32 对应 512)
    # clip_feature_dim: 512

#####################################
# 日志与保存设置 (Logger & Checkpoint Settings)
#####################################
LOGGER:
  SACE_CHECKPOINT_EPOCH: 20 # 每 100 个 epoch 保存一次模型
  LOG_EVERY_STEPS: 1
  VAL_EVERY_STEPS: 50 # 每 500 个训练步，运行一次验证
  TENSORBOARD: True
  WANDB:
    PROJECT: null # 如果你用 wandb，请填写项目名，例如 "mld-finetune"
    OFFLINE: True # 设为 False 以在线记录
    RESUME_ID: null

# --- 其他配置 (LOSS, METRIC, TEST 等) 保持与原始文件一致即可 ---
# --- 为完整性，我将它们也复制过来 ---

METRIC:
  TYPE: ['TemosMetric', 'TM2TMetrics']

LOSS:
  TYPE: mld
  # [新代码] 为我们的文本引导损失添加一个权重
  # LAMBDA_TEXT_STYLE: 1.5
  LAMBDA_ALIGN: 0.06  # 本来是1.0，现在用0.06，为了和inst loss做尺度上的对齐
  LAMBDA_STYLE_RECON: 1.0  # 感知损失的最大权重,本来是10.0，效果比较差，改一下参数
  STYLE_RECON_START_STEP: 500000 # 5000 # 训练多少步之后，才开始计算感知损失.如果是resume的话可以是15000左右，但是如果是从某个pretrained权重开始的，那就要从0开始
  STYLE_RECON_INTERVAL: 1000 # 每隔多少步计算一次感知损失
  LAMBDA_LATENT: 1.0e-5
  LAMBDA_KL: 1.0e-4
  LAMBDA_REC: 1.0
  LAMBDA_GEN: 1.0
  LAMBDA_CROSS: 1.0
  LAMBDA_CYCLE: 0.0
  LAMBDA_PRIOR: 0.0
  DIST_SYNC_ON_STEP: False

TEST:
  CHECKPOINTS: '' # 微调后，将这里指向你最好的新 checkpoint
  DATASETS: ['humanml3d']
  SPLIT: test
  BATCH_SIZE: 256

# DEMO 设置 (用于驱动 demo.py)
#####################################
DEMO:
  # [关键] 指定你要测试的、训练好的 checkpoint 文件！
  CHECKPOINT: "/root/autodl-tmp/MyRepository/MCM-LDM/experiments/mld/DualMode_MLD_Finetune_v1_1105/checkpoints/best-epoch=69-val/total_loss=0.34.ckpt" # 训练10个epoch后，这里可能是 epoch=9.ckpt

  # --- 模式一：文本引导 ---
  # [关键] 如果要使用文本引导，请在这里填入文本。
  # 如果留空 ("") ，则脚本会自动进入动作引导模式。
  # STYLE_TEXT: "" # Arms out to side for balance - tilting side to side
  # STYLE_TEXT: "reach his left arm very high."
  STYLE_TEXT: "elder man, bow" 
  # STYLE_TEXT: "normal motion with the upper body tilted to the right"
  # STYLE_TEXT: "like a proud robot, big steps and kick"

  # --- 模式二：动作引导 ---
  # 仅在 STYLE_TEXT 为空时生效
  STYLE_MOTION_DIR: "demo/style_motion/" # 存放风格动作 .npy 文件的目录
  CONTENT_MOTION_DIR: "demo/content_motion/" # 存放内容动作 .npy 文件的目录

  # --- 其他参数 ---
  SCALE: 5.0 # guidance_scale, 可以尝试调整这个值 (例如 2.5, 7.5) 观察效果
